{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landsat_C2_ST_manu_v6.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DBZzaGgqpr8W"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steeleb/ids-ne-lakes/blob/master/colab_notebooks/landsat_C2_ST_manu_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr9ipVEDv7BK"
      },
      "source": [
        "**Version 6**<br>\n",
        "*Last updated: 2021-11-12<p>*\n",
        "christina.herrick@unh.edu<br>\n",
        "steeleb@caryinstitute.org<br>\n",
        "__Please do not distribute__\n",
        "\n",
        "Include kurtosis measure in scene stats; rename stats columns so that they are more clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psnrRlTjBj2M"
      },
      "source": [
        "# **Initial Setup**\n",
        "\n",
        "This notebook uses the [Google Earth Engine](https://developers.google.com/earth-engine) [Python API](https://developers.google.com/earth-engine/guides/python_install) to extract the surface temperature product from Landsat 4, 5, 7, and 8 Collection 2 images for any water body in the world. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3zRra91DaEq"
      },
      "source": [
        "## Modules\n",
        "\n",
        "This section of code blocks imports necessary python modules for the notebook to run. You will be prompted to click one or more URLs and be taken to a page to sign in with your Google account. This account must already be authorized to use Google Earth Engine. If you do not already have access, [fill out this application](https://signup.earthengine.google.com/#!/).\n",
        "\n",
        "Copy the unique code(s) provided on the web page when prompted and paste where prompted to finish authorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh-VYdMKCAkM",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to authorize Colab to authenticate your Google account and give it access to upload files from your local computer ([to be used later](https://colab.research.google.com/drive/1AJFnCB7B7Uev2-0hqIayOb5fkSB0Xn8v#scrollTo=L_Tf54wwIPRy) in the notebook). Once you paste the code, press Enter.\n",
        "from google.colab import auth, files\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STd1_lFcVYMZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to connect Colab to your Earth Engine account\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaB_g_reJG41",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to connect Colab to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SN2nSfHBF1S",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to install packages and functions used for \n",
        "#@markdown interactive mapping and data exploration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strftime, sleep\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import folium\n",
        "from folium import plugins\n",
        "from google.colab import data_table\n",
        "\n",
        "# Add custom basemaps to folium\n",
        "basemaps = {\n",
        "    'Google Maps': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Maps',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Terrain': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=p&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Terrain',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Google Satellite Hybrid': folium.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    ),\n",
        "    'Esri Satellite': folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Satellite',\n",
        "        overlay = True,\n",
        "        control = True\n",
        "    )\n",
        "}\n",
        "\n",
        "# Define a method for displaying Earth Engine image tiles on a folium map.\n",
        "def add_ee_layer(self, ee_object, vis_params, name):\n",
        "    \n",
        "    try:    \n",
        "        # display ee.Image()\n",
        "        if isinstance(ee_object, ee.image.Image):    \n",
        "            map_id_dict = ee.Image(ee_object).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.ImageCollection()\n",
        "        elif isinstance(ee_object, ee.imagecollection.ImageCollection):    \n",
        "            ee_object_new = ee_object.mosaic()\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "            ).add_to(self)\n",
        "        # display ee.Geometry()\n",
        "        elif isinstance(ee_object, ee.geometry.Geometry):    \n",
        "            folium.GeoJson(\n",
        "            data = ee_object.getInfo(),\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "        # display ee.FeatureCollection()\n",
        "        elif isinstance(ee_object, ee.featurecollection.FeatureCollection):  \n",
        "            ee_object_new = ee.Image().paint(ee_object, 0, 2)\n",
        "            map_id_dict = ee.Image(ee_object_new).getMapId(vis_params)\n",
        "            folium.raster_layers.TileLayer(\n",
        "            tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "            attr = 'Google Earth Engine',\n",
        "            name = name,\n",
        "            overlay = True,\n",
        "            control = True\n",
        "        ).add_to(self)\n",
        "    \n",
        "    except:\n",
        "        print(\"Could not display {}\".format(name))\n",
        "    \n",
        "# Add EE drawing method to folium.\n",
        "folium.Map.add_ee_layer = add_ee_layer\n",
        "\n",
        "print(\"Packages installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4zmuTIA8SR"
      },
      "source": [
        "## Imported variables\n",
        "These are Landsat-specific variable settings for GEE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkiZcezXBEG5",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to import pre-existing features, images, and collections from Earth Engine\n",
        "wrs2 = ee.FeatureCollection(\"users/christinaherrickunh/WRS2_descending_2018\")\n",
        "utmbounds = ee.FeatureCollection(\"users/christinaherrickunh/UTM_Zone_Boundaries\")\n",
        "sw = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
        "l4t1 = ee.ImageCollection(\"LANDSAT/LT04/C02/T1_L2\")\n",
        "l4t2 = ee.ImageCollection(\"LANDSAT/LT04/C02/T2_L2\")\n",
        "l5t1 = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\")\n",
        "l5t2 = ee.ImageCollection(\"LANDSAT/LT05/C02/T2_L2\")\n",
        "l7t1 = ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\")\n",
        "l7t2 = ee.ImageCollection(\"LANDSAT/LE07/C02/T2_L2\")\n",
        "l8t1 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
        "l8t2 = ee.ImageCollection(\"LANDSAT/LC08/C02/T2_L2\")\n",
        "\n",
        "print(\"variables installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxgpEciUBKLy"
      },
      "source": [
        "## Define the lake area\n",
        "\n",
        "In this section, use either *Option A*, where you define the bounding box, or *Option B* where you import a shape file of your lake. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiuNetjpgpJ"
      },
      "source": [
        "###Option A: use a bounding box\n",
        "If you need help finding your coordinates, try using [OpenStreetMap](https://www.openstreetmap.org/export#map=12/43.3826/-72.0157). The bounding box should be as small as possible while still including the entire lake surface.<p>\n",
        "Enter coordinates to see a map of the bounding box. You can also click on the map to get interactive pop-ups of the latitude (y) and longitude (x), and replace/re-run the coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQX8qmNz_aLM",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ after inputting your coordinates below.\n",
        "#@markdown Do not run this block if you are using Option B\n",
        "\n",
        "#@markdown `lat1` = North\n",
        "lat1 = '43.4320'  #@param {type:\"string\"}\n",
        "#@markdown `lat2` = South\n",
        "lat2 = '43.3162'  #@param {type:\"string\"}\n",
        "#@markdown `lon1` = West\n",
        "lon1 = '-72.0923'  #@param {type:\"string\"}\n",
        "#@markdown `lon2` = East\n",
        "lon2 = '-72.0154'  #@param {type:\"string\"}\n",
        "\n",
        "use_user_input_file = \"no\"\n",
        "try:\n",
        "  lat1 = float(lat1)\n",
        "  lon1 = float(lon1)\n",
        "  lat2 = float(lat2)\n",
        "  lon2 = float(lon2)\n",
        "except ValueError as e:\n",
        "  raise ValueError(\"Coordinates are missing or are not numbers\")\n",
        "box = ee.Geometry.Polygon(\n",
        "    [[[lon1, lat1], # ur\n",
        "      [lon2, lat1], # ul\n",
        "      [lon2, lat2], # ll\n",
        "      [lon1, lat2]]], None, False) # lr\n",
        "center_lat = (lat1+lat2)/2\n",
        "center_lon = (lon1+lon2)/2\n",
        "locs_fig = folium.Figure(width=\"40%\")\n",
        "locs_map = folium.Map(location=[center_lat,center_lon],zoom_start=10).add_to(locs_fig)\n",
        "locs_map.add_child(folium.LatLngPopup())\n",
        "basemaps['Google Maps'].add_to(locs_map)\n",
        "locs_map.add_ee_layer(box,{},'aoi')\n",
        "display(locs_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZzaGgqpr8W"
      },
      "source": [
        "###Option B: upload a table file\n",
        "Using your Google Earth Engine account, upload a shapefile or csv of your study lake to your Assets and link to it here.<p>Click [here](https://drive.google.com/file/d/1lfFoZzQD_wAA7Bnil7mI4zwjKDG5kegh/view?usp=sharing) and [here](https://drive.google.com/file/d/1KNaZV63J_LUp6X1OcoLX1wonF0ASiIzi/view?usp=sharing) for screenshots of the process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0lONKBvqAUp",
        "cellView": "form"
      },
      "source": [
        "#@markdown File path should begin with 'users/'\n",
        "user_input_file = \"\" #@param {type: \"string\"}\n",
        "use_user_input_file = \"no\" #@param [\"yes\",\"no\"] {allow-input: false}\n",
        "#@markdown Remember to __run this block__ after filling in the above fields.\n",
        "#@markdown Do not run this block if you are using Option A."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIKkHHMnKqik",
        "cellView": "form"
      },
      "source": [
        "#@markdown For Opt B, __run this block__ to calculate the centroid latitude and longitude\n",
        "if len(user_input_file)>1:\n",
        "  shp = ee.FeatureCollection(user_input_file).geometry()\n",
        "  centroid = shp.centroid(10).getInfo()[\"coordinates\"]\n",
        "  center_lat = centroid[1]\n",
        "  center_lon = centroid[0]\n",
        "  print(center_lat)\n",
        "  print(center_lon)\n",
        "else:\n",
        "  print(\"No file input; will use Opt A bounding box coordinates\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93P5zojCKmwm",
        "cellView": "form"
      },
      "source": [
        "#@markdown For Opt B, __run this block__ to confirm the waterbody AOI\n",
        "if 'shp' in globals() or 'shp' in locals():\n",
        "  shp_map = folium.Map(location=[center_lat,center_lon],zoom_start=10,width=600,height=600)\n",
        "  shp_map.add_child(folium.LatLngPopup())\n",
        "  basemaps['Google Maps'].add_to(shp_map)\n",
        "  shp_map.add_ee_layer(shp,{},'aoi')\n",
        "  display(shp_map)\n",
        "else:\n",
        "  print(\"Display does not apply here; Opt A bounding box is being used for AOI\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDrkGq_jwgIo"
      },
      "source": [
        "## Set remaining user variables\n",
        "\n",
        "This section defines acceptable water persistence and error for the Landsat pixels, the time period of interest, and confirms the path and row for Landsat acquisition. You can also change the default Google Drive folder for any outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5s5K2jQm9Zt",
        "cellView": "form"
      },
      "source": [
        "#@markdown The default path for any exports is in the Colab Notebooks directory \n",
        "#@markdown within Google Drive. If preferred, it can be changed here\n",
        "output_dir = \"/content/drive/MyDrive/NASA Landsat Temperature Product/Colab Output/C2_20211112\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Please indicate the name of the final foler in your output directory:\n",
        "short_dir = \"C2_20211112\"#@param {type:'string'}\n",
        "#@markdown Enter a file naming prefix for any exports (no spaces or special characters)\n",
        "file_prefix = \"C2_v2\" #@param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "def check_splcharacter(test):\n",
        "  string_check = re.compile('[@!#$%^&*() <>?/\\|}{~:]')\n",
        "  if not string_check.search(test)==None:\n",
        "    print(\"Your file prefix contains special characters, please fix\")\n",
        "    file_prefix = \"your_lake_name\"\n",
        "  else:\n",
        "    print(\"File prefix:\",test)\n",
        "\n",
        "check_splcharacter(file_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhZWCrkVrb0",
        "cellView": "form"
      },
      "source": [
        "#@markdown Boundaries of water bodies are automatically detected using the JRC \n",
        "#@markdown Global Surface Water Mapping Layer dataset of percent water occurrence. \n",
        "#@markdown By default, a pixel has to be classified as water at least 55% of the \n",
        "#@markdown time. This is defined as `pctTime`\n",
        "\n",
        "#@markdown <p>More information on this dataset can be found at https://doi.org/10.1038/nature20584\n",
        "\n",
        "pctTime = 55  #@param {type: \"number\"}\n",
        "#@markdown __Run this block__ after inputting the above parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9WlKk7iHcRC",
        "cellView": "form"
      },
      "source": [
        "#@markdown Landsat scenes from both Tier 1 and Tier 2 of Collection 2 are \n",
        "#@markdown included for possible use. From Tier 2, only scenes that are processed \n",
        "#@markdown to L1T with a combined RMS error of no greater than 30 meters are \n",
        "#@markdown considered.\n",
        "rmse = 24  #@param {type: \"number\"}\n",
        "#@markdown __Run this block__ after inputting the above parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJaiep9n1F7",
        "cellView": "form"
      },
      "source": [
        "#@markdown Landsat records begin in 1984 with Landsat 4 TM. \n",
        "#@markdown Enter the year range that you want to search, as well as months of \n",
        "#@markdown the year. To search all months, `m1 = 1` and `m2 = 12` <p>\n",
        "#@markdown <p>First year (y1) and last year (y2). Years are inclusive.<p>\n",
        "#@markdown <p>First month (m1) and last month (m2). Months are inclusive.<p>\n",
        "#@markdown <p>Note that this method has only been developed for lake ice-off temperature estimation. \n",
        "\n",
        "y1 = 1980 #@param {type: \"number\"}\n",
        "y2 = 2020 #@param {type: \"number\"}\n",
        "\n",
        "m1 = 5 #@param {type: \"number\"}\n",
        "m2 =  11#@param {type: \"number\"}\n",
        "print('Script will search years %d through %d and months %d through %d of each year' % (y1,y2,m1,m2))\n",
        "#@markdown __Run this block__ after inputting the above parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stjrJlU2HamN",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to calculate the Landsat path(s) and row(s) that \n",
        "#@markdown overlap the extent of Option A/B It will print as\n",
        "#@markdown `WRS2 Path/Row: PPPRRR` where `PPP` is the three-digit path, and \n",
        "#@markdown `RRR` is the three-digit row. This block also determines the correct\n",
        "#@markdown CRS for the GEE analyses.\n",
        "\n",
        "if use_user_input_file==\"yes\":\n",
        "  try:\n",
        "    box = shp\n",
        "  except NameError:\n",
        "    raise NameError(\"Cannot find user-input file. Are you using Option B or Opt A bounding box? You may need to correct and rerun.\")\n",
        "\n",
        "pathrow = wrs2.filterBounds(box)\n",
        "  \n",
        "num_of_pr = len(pathrow.getInfo()[\"features\"])\n",
        "path_west = 1\n",
        "path_east = 233\n",
        "row_north = 122\n",
        "row_south = 1\n",
        "\n",
        "for i in range(0,num_of_pr):\n",
        "  pr = pathrow.getInfo()[\"features\"][i][\"properties\"][\"PR\"]\n",
        "  p = pr[:3]\n",
        "  r = pr[3:]\n",
        "  if int(p) > path_west:\n",
        "    path_west = int(p)\n",
        "  if int(p) < path_east:\n",
        "    path_east = int(p)\n",
        "  if int(r) < row_north:\n",
        "    row_north = int(r)\n",
        "  if int(r) > row_south:\n",
        "    row_south = int(r)\n",
        "\n",
        "print('Google Earth Engine settings:')\n",
        "p1 = path_west\n",
        "p2 =  path_east\n",
        "r1 =  row_north\n",
        "r2 =  row_south\n",
        "print(f'\\nbounding paths: {path_west} to {path_east}')\n",
        "print(f'bounding rows: {row_north} to {row_south}')\n",
        "\n",
        "utm = utmbounds.filterBounds(box)\n",
        "\n",
        "#indicate UTM zone\n",
        "if not len(utm.getInfo()[\"features\"])>1:\n",
        "  printUTM = utm.getInfo()[\"features\"][0][\"properties\"][\"ZONE\"]\n",
        "  printNS = utm.getInfo()[\"features\"][0][\"properties\"][\"HEMISPHERE\"]\n",
        "  print(\"UTM Zone:\",printUTM,printNS)\n",
        "else:\n",
        "  print(utm.getInfo()[\"features\"][1][\"properties\"][\"ZONE\"],\"(AOI overlaps two UTM zones)\")\n",
        "  print(printUTM,\"will be used for projected CRS\")\n",
        "  printNS = utm.getInfo()[\"features\"][1][\"properties\"][\"HEMISPHERE\"]\n",
        "\n",
        "#indicate hemisphere  \n",
        "if printNS == 'n':\n",
        "  crs_out = f\"EPSG:326{printUTM}\"\n",
        "else:\n",
        "  crs_out = f\"EPSG:327{printUTM}\"\n",
        "print(\"CRS:\",crs_out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0idqpc4FKCE1"
      },
      "source": [
        "#@markdown __Run this block__ to confirm path and row overlap with your lake. Your lake will be highlighted in a blue box.\n",
        "zone_map = folium.Map(location=[center_lat,center_lon],zoom_start=8,width=600,height=600)\n",
        "zone_map.add_child(folium.LatLngPopup())\n",
        "basemaps['Google Maps'].add_to(zone_map)\n",
        "zone_map.add_ee_layer(pathrow,{},'path row')\n",
        "zone_map.add_ee_layer(box,{},'aoi box')\n",
        "display(zone_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xE7MPgvY78u"
      },
      "source": [
        "# **Find water pixels & delineate boundary**\n",
        "This step uses the bounding box or the table file as well as the water persistence using [JRC Water dataset](https://global-surface-water.appspot.com/) to define the waterbody area. \n",
        "\n",
        "Note, in some cases the _Opt A_ bounding box method will include upstream and downstream water areas, particularily where there are rivers that are large enough to be detected by Landsat. To avoid this, use the _Opt B_ table file method. <p>\n",
        "You can explore the JRC Water dataset [here](https://global-surface-water.appspot.com/).<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ljgJffQF1Y",
        "cellView": "form"
      },
      "source": [
        "#@markdown This code block reduces the bounding box defined above to the largest water body present, and then counts the 30m pixels within the water body.<p>\n",
        "#@markdown Landsat scenes included in analysis must have a minimum percentage of clear lake pixels available (these are discontinuous pixels)\n",
        "pctLakeCoverage = 25 #@param\n",
        "#@markdown __Run this block__ after inputting the above parameter\n",
        "\n",
        "wateroccurrence = sw.select(0).setDefaultProjection(crs_out);\n",
        "water = wateroccurrence.gte(pctTime)\n",
        "water = water.updateMask(water.neq(0))\n",
        "\n",
        "def addArea(feature):\n",
        "  # returns area +/- 1sqm\n",
        "  return feature.set({'area':feature.geometry().area(1)})  \n",
        "\n",
        "regions = water.addBands(wateroccurrence).reduceToVectors(\n",
        "    reducer=ee.Reducer.min(),\n",
        "    geometry=box,\n",
        "    scale=30,\n",
        "    maxPixels=5e9,\n",
        "    geometryInNativeProjection=False,\n",
        "    labelProperty='surfaceWater').map(addArea).sort('area',False);\n",
        "\n",
        "lake_outline = ee.Feature(regions.first())\n",
        "aoi = lake_outline\n",
        "geo = lake_outline.geometry()\n",
        "coords = geo.getInfo()['coordinates'][0]\n",
        "\n",
        "watercount = water.reduceRegion(reducer=ee.Reducer.count().unweighted(), \n",
        "                                geometry=lake_outline.geometry(),\n",
        "                                scale=30, bestEffort=True)\n",
        "totalpixels = watercount.getInfo()[\"occurrence\"]\n",
        "pixel_min = totalpixels*(pctLakeCoverage/100.0)\n",
        "print(\"\\n\\nTotal # of lake pixels: \", totalpixels);\n",
        "print(f\"{pctLakeCoverage}% of available lake pixels is\", int(pixel_min))\n",
        "\n",
        "pixel_fig = folium.Figure(width=\"50%\")\n",
        "pixel_map = folium.Map(location=[center_lat,center_lon],zoom_start=11).add_to(pixel_fig)\n",
        "basemaps['Google Maps'].add_to(pixel_map)\n",
        "display(pixel_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45tElCUuotvD"
      },
      "source": [
        "# **Get Skin Surface Temperatures**\n",
        "Using all of the information amassed so far, the code blocks in this section perform all necessary steps to get skin temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbd6m5SRlsyF",
        "cellView": "form"
      },
      "source": [
        "# @title Landsat functions\n",
        "# @markdown __Run this block__ to define the functions that pre-process\n",
        "# @markdown Landsat images.\n",
        "def prep457bands(img):\n",
        "  systime = img.get('system:time_start')\n",
        "  elev = img.get(\"SUN_ELEVATION\")\n",
        "  sza = ee.Number(90).subtract(elev)\n",
        "  \n",
        "  qa = img.select([\"QA_PIXEL\"])\n",
        "  cloud1 = qa.bitwiseAnd(2).eq(0)  # bit 1, dilated cloud\n",
        "  cloud3 = qa.bitwiseAnd(8).eq(0)  # bit 3, cloud\n",
        "  snow = qa.bitwiseAnd(32).eq(0)  # bit 5, snow\n",
        "  cloud_confid = qa.rightShift(8).bitwiseAnd(3).lt(2)  # bits 8-9\n",
        "  snow_confid = qa.rightShift(12).bitwiseAnd(3).lt(2)  # bits 12-13\n",
        "  cirrus_confid = qa.rightShift(14).bitwiseAnd(3).lt(2)  # bits 14-15\n",
        "\n",
        "  temp = img.select([\"ST_B6\"],[\"surface_temp\"]).multiply(0.00341802).add(149).add(-273.15)\n",
        "  \n",
        "  updated = (temp.updateMask(cloud1).updateMask(cloud3).updateMask(snow)\n",
        "  .updateMask(cloud_confid).updateMask(snow_confid).updateMask(cirrus_confid))\n",
        "          \n",
        "  return ee.Image(updated).copyProperties(img).set(\"system:time_start\",systime).set(\"SOLAR_ZENITH_ANGLE\", sza)\n",
        "\n",
        "def prep8bands(img):\n",
        "  systime = img.get('system:time_start')\n",
        "  elev = img.get(\"SUN_ELEVATION\")\n",
        "  sza = ee.Number(90).subtract(elev)\n",
        "\n",
        "  qa = img.select([\"QA_PIXEL\"])\n",
        "  cloud1 = qa.bitwiseAnd(2).eq(0)  # bit 1, dilated cloud\n",
        "  cloud2 = qa.bitwiseAnd(4).eq(0)  # bit 2, cirrus (high confidence)\n",
        "  cloud3 = qa.bitwiseAnd(8).eq(0)  # bit 3, cloud\n",
        "  snow = qa.bitwiseAnd(32).eq(0)  # bit 5, snow\n",
        "  cloud_confid = qa.rightShift(8).bitwiseAnd(3).lt(2)  # bits 8-9\n",
        "  snow_confid = qa.rightShift(12).bitwiseAnd(3).lt(2)  # bits 12-13\n",
        "  cirrus_confid = qa.rightShift(14).bitwiseAnd(3).lt(2)  # bits 14-15\n",
        "  \n",
        "  temp = img.select([\"ST_B10\"],[\"surface_temp\"]).multiply(0.00341802).add(149).add(-273.15)\n",
        "  updated = (temp.updateMask(cloud1).updateMask(cloud3).updateMask(snow).\n",
        "             updateMask(cloud_confid).updateMask(snow_confid).updateMask(cirrus_confid))\n",
        "\n",
        "  return ee.Image(updated).copyProperties(img).set(\"system:time_start\",systime).set(\"SOLAR_ZENITH_ANGLE\", sza)\n",
        "\n",
        "\n",
        "print(\"Functions imported\", strftime(\"%x %X\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TjyRw18ttpV",
        "cellView": "form"
      },
      "source": [
        "# @title Filter and Stack Landsat\n",
        "\n",
        "# @markdown __Run this block__ to filter by date and site location, \n",
        "# @markdown then convert temperature from K --> C, stack them together, and make \n",
        "# @markdown sure the metadata and system time carries over. For landsat 8, make \n",
        "# @markdown sure scenes are nadir and the TIRS algorithm isn't preliminary version.\n",
        "\n",
        "# @markdown Filter by solar zenith angle? This is useful in high-latitude areas\n",
        "filter_sza = \"yes\" #@param [\"yes\",\"no\"] {allow-input: false}\n",
        "\n",
        "l4 = (l4t1.merge(l4t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l5 = (l5t1.merge(l5t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l7 = (l7t1.merge(l7t2).filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep457bands, True))\n",
        "\n",
        "l8 = (l8t1.merge(l8t2).filterMetadata('NADIR_OFFNADIR','equals','NADIR') \\\n",
        "      .filterMetadata('TIRS_SSM_MODEL','not_equals','PRELIMINARY') \\\n",
        "      .filterMetadata('L1_PROCESSING_LEVEL','equals','L1TP') \\\n",
        "      .filterMetadata('GEOMETRIC_RMSE_MODEL','not_greater_than',rmse) \\\n",
        "      .filterBounds(geo) \\\n",
        "      .filter(ee.Filter.calendarRange(y1,y2,'year')) \\\n",
        "      .filter(ee.Filter.calendarRange(m1,m2,'month')) \\\n",
        "      .filterMetadata('WRS_ROW','not_less_than',r1).filterMetadata('WRS_ROW','not_greater_than',r2) \\\n",
        "      .filterMetadata('WRS_PATH','not_greater_than',p1).filterMetadata('WRS_PATH','not_less_than',p2) \\\n",
        "      .map(prep8bands, True))\n",
        "\n",
        "if filter_sza==\"yes\":\n",
        "  landsat = ee.ImageCollection((l4).merge(l5).merge(l7).merge(l8)).filterMetadata('SOLAR_ZENITH_ANGLE','less_than',77).sort('system:time_start')\n",
        "else:\n",
        "  landsat = ee.ImageCollection((l4).merge(l5).merge(l7).merge(l8)).sort('system:time_start')\n",
        "\n",
        "print(\"Landsat compiled at\", strftime(\"%x %X\"))\n",
        "\n",
        "'''\n",
        "Add metadata to each scene that indicates how many visible pixels were included \n",
        "in analysis, and filter images so that only scenes with the minimum number of \n",
        "pixels remain.\n",
        "'''\n",
        "def countPixels(img):\n",
        "  img = ee.Image(img)\n",
        "  getCount = img.reduceRegion(**{\n",
        "      \"reducer\": ee.Reducer.count(),\n",
        "      \"geometry\": geo, \n",
        "      \"scale\": 30})\n",
        "  count = ee.Dictionary(getCount).get('surface_temp')\n",
        "  return img.set('pixel_count',count)\n",
        "before = landsat.map(countPixels)\n",
        "countedPixels = before.filterMetadata('pixel_count','not_less_than', pixel_min)\n",
        "print('Total number of LS scenes:',len(before.getInfo()['features']),'\\nNumber of LS scenes after filter:', len(countedPixels.getInfo()['features']))\n",
        "print(\"Landsat filtered:\", strftime(\"%x %X\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FleAQ6cyfch"
      },
      "source": [
        "# **Data Visualization**\n",
        "The code blocks in the following section allow you to explore your results using dataframes and pixel-value distributions. You can visually inspect each scene for errant data or export the data to explore further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8K6XN2xZMjP",
        "cellView": "form"
      },
      "source": [
        "#@title Convert data from server-side EE objects to client-side Python objects\n",
        "#@markdown __Run this block__ to define functions for data exploration\n",
        "\n",
        "\n",
        "# These functions convert image pixels to numpy arrays\n",
        "def createArrays(img):\n",
        "  psi_prop = ee.Image(img).sampleRectangle(region=geo, defaultValue=-1)\n",
        "  getProp = psi_prop.get(\"surface_temp\")\n",
        "  return img.set(\"img_array\", getProp)\n",
        "\n",
        "def getArrays(client_img_col):\n",
        "  list_of_arrays = []\n",
        "  for img in client_img_col:\n",
        "    prop = img[\"properties\"][\"img_array\"]\n",
        "    a = np.array(prop)\n",
        "    list_of_arrays.append(a)\n",
        "  return list_of_arrays\n",
        "print('Conversion function imported.')\n",
        "\n",
        "def generate_histogram(img):\n",
        "  img = ee.Image(img)\n",
        "  fhisto = img.reduceRegion(**{\n",
        "      \"reducer\": ee.Reducer.fixedHistogram(-5,30,350).unweighted(),\n",
        "      \"geometry\": geo,\n",
        "      \"scale\": 30,\n",
        "      \"maxPixels\": 5e9\n",
        "  })\n",
        "  return img.set('histogram',fhisto.get('surface_temp'))\n",
        "print(\"Will generate histograms with a fixed x-axis between -5 - 30 deg C in 0.1 degree bins\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InafBTf5ZvAS",
        "cellView": "form"
      },
      "source": [
        "# @markdown This code block executes the functions from the previous block and \n",
        "# @markdown converts the image collection from a server-side EE object to a client-side \n",
        "# @markdown Python object. This allows iteration over the image collection with a `for` \n",
        "# @markdown loop, which provides much more Python functionality.<p><mark>__This step \n",
        "# @markdown could take some time to run</mark> depending on the length of the \n",
        "# @markdown ImageCollection.__\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "imgs_histo = countedPixels.map(generate_histogram)\n",
        "generate_arrays = imgs_histo.map(createArrays)\n",
        "\n",
        "imgCol = generate_arrays.getInfo()[\"features\"]\n",
        "print(\"Finish:\", strftime(\"%x %X\"))\n",
        "print(\"Number of Landsat scenes: \", len(imgCol))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-cobA_BbwRK",
        "cellView": "form"
      },
      "source": [
        "#@markdown Now that the image collection is client-side, __run this block__ to \n",
        "#@markdown get arrays of each image.\n",
        "# returns a list of arrays\n",
        "imgs_to_arrays = getArrays(imgCol)\n",
        "print(\"Converted images to arrays\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9sJyjsi9eCF",
        "cellView": "form"
      },
      "source": [
        "#@title View and export collated pixel distributions\n",
        "#@markdown __Run this block__ to iterate through the image collection and convert \n",
        "#@markdown all bins and frequencies to a single Pandas `DataFrame`. \n",
        "#@markdown This returns *bins* (in deg C) in the first column, the image date as \n",
        "#@markdown all subsequent column headers, and the pixel counts within each bin.\n",
        "\n",
        "length_plots = len(imgCol)\n",
        "list_of_dfs = []\n",
        "list_of_uids = []\n",
        "# fig = make_subplots(rows=length_plots, cols=1)\n",
        "for i in imgCol:\n",
        "  uid = (i['properties']['L1_LANDSAT_PRODUCT_ID'])\n",
        "  list_of_uids.append(uid)\n",
        "  histogram = i[\"properties\"].get(\"histogram\")\n",
        "\n",
        "  a = pd.DataFrame(histogram, columns=[\"bin\",uid]).set_index(\"bin\")\n",
        "  # fig.add_histogram()\n",
        "  list_of_dfs.append(a)\n",
        "\n",
        "appended_dfs = pd.concat(list_of_dfs,ignore_index=False, axis=1)\n",
        "transposed_dfs = appended_dfs.T\n",
        "data_table.DataTable(appended_dfs, include_index=True, max_columns=50, max_rows=50, num_rows_per_page=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70VebGE_KA--",
        "cellView": "form"
      },
      "source": [
        "#@markdown __Run this block__ to export the `DataFrame` as a csv to your Google Drive\n",
        "\n",
        "if not 'output_dir' in globals():\n",
        "  output_dir = \"drive/My Drive/Colab Notebooks/\"\n",
        "out_histo_compiled = os.path.join(output_dir, file_prefix + \"_histograms.csv\")\n",
        "appended_dfs.to_csv(out_histo_compiled)\n",
        "print(\"Export completed:\", out_histo_compiled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_AbqwwDcT9j",
        "cellView": "form"
      },
      "source": [
        "#@title View pixel distribution using `matplotlib`\n",
        "#@markdown __Run this block__ to generate histograms for each lake temperature \n",
        "#@markdown Landsat scene. Each scene is saved as png in a folder 'histograms' in your specified output directory. \n",
        "#@markdown This step can also take some time to run.\n",
        "\n",
        "#print(len(list_of_uids),\" histograms will be generated\")\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "\n",
        "#make 'histogram' folder in defined directory\n",
        "if not os.path.exists(os.path.join(output_dir, 'histograms')):\n",
        "    os.makedirs(os.path.join(output_dir, 'histograms'))\n",
        "\n",
        "histo_dir = os.path.join(output_dir, 'histograms')\n",
        "\n",
        "print(\"Histograms will be saved at \", histo_dir)\n",
        "\n",
        "#reorient data\n",
        "histos = appended_dfs.set_index('bin')\n",
        "#list of scenes\n",
        "scenes = histos.columns\n",
        "histos = pd.DataFrame.transpose(histos)\n",
        "#reset index\n",
        "histos = histos.reset_index()\n",
        "histos_vert = histos.melt(id_vars=['index'])\n",
        "\n",
        "#iterate over the scene list\n",
        "for i in range(0, len(scenes)):\n",
        "  df = histos_vert[histos_vert['index'] == scenes[i]]\n",
        "  histoname = os.path.join(histo_dir,scenes[i]+'_histo.png')\n",
        "  plt.bar(x = df.bin, height = df.value)\n",
        "  plt.ylabel('frequency')\n",
        "  plt.xlabel('temperature (deg C)')\n",
        "  plt.title(scenes[i])\n",
        "  plt.savefig(histoname)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "print(\"Finish:\", strftime(\"%x %X\"), 'Your histograms should now be visible at ', histo_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1sYNsf_ndyG"
      },
      "source": [
        "#Export to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9STTRpLaRnD",
        "cellView": "form"
      },
      "source": [
        "#@markdown Execute this cell block to export a CSV containing temperature and related image metadata\n",
        "\n",
        "def exportWholeLakeStats(img):\n",
        "  img = ee.Image(img).clip(geo)\n",
        "\n",
        "  # retrieve image metadata for output file\n",
        "  landsattime = img.get('system:time_start')\n",
        "  cloudcover = img.get('CLOUD_COVER')\n",
        "  esd = img.get(\"EARTH_SUN_DISTANCE\")\n",
        "  elev = img.get('SUN_ELEVATION')\n",
        "  azi = img.get('SUN_AZIMUTH')\n",
        "  sza = img.get('SOLAR_ZENITH_ANGLE')\n",
        "  count = img.get('pixel_count')\n",
        "  pctAvail = ee.Number(count).divide(totalpixels)\n",
        "  #flags = ee.List([])\n",
        "\n",
        "  # get statistics for skin_temperature\n",
        "  reducers = ee.Reducer.mean().combine(**{\n",
        "      \"reducer2\": ee.Reducer.stdDev(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.minMax(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.median(**{\"maxBuckets\":500, \"minBucketWidth\": 0.125}), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.skew(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.kurtosis(), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.percentile(**{\"percentiles\": [25,75], \"maxBuckets\": 500, \"minBucketWidth\": 0.125}), \"sharedInputs\":True}).combine(**{\n",
        "      \"reducer2\": ee.Reducer.count(), \"sharedInputs\":True})\n",
        "  \n",
        "  stats = img.reduceRegion(**{\n",
        "      \"reducer\": reducers,\n",
        "      \"geometry\": geo,\n",
        "      \"scale\": 30,\n",
        "      \"maxPixels\": 5e9\n",
        "  })\n",
        "  \n",
        "  more_stats = ee.Dictionary({'pixel_count':count,\n",
        "                            'landsat_time':landsattime,\n",
        "                            'cloud_cover':cloudcover,\n",
        "                            'elev':elev,\n",
        "                            'azimuth':azi,\n",
        "                            'esd':esd,\n",
        "                            'sza':sza,\n",
        "                            'pct_lake':pctAvail,\n",
        "                            'l_exceltime':ee.Number(landsattime).divide(1000.0).divide(86400).add(25569)\n",
        "                            })\n",
        "  stats2 = ee.Dictionary.combine(stats, more_stats)\n",
        "\n",
        "  return ee.Feature(None,stats2)\n",
        "\n",
        "temp_stats = countedPixels.map(exportWholeLakeStats)\n",
        "\n",
        "#create the file name with user-specified prefix\n",
        "temp_filename = (file_prefix+'_temp_stats')\n",
        "\n",
        "export_task = ee.batch.Export.table.toDrive(**{\n",
        "    'collection': temp_stats,\n",
        "    'description': temp_filename,\n",
        "    \"fileFormat\": \"CSV\",\n",
        "    'folder': short_dir\n",
        "})\n",
        "\n",
        "print(\"Exporting \", temp_filename+\".csv\")\n",
        "export_task.start()\n",
        "print('Polling for task (id: {}) at'.format(export_task.id))\n",
        "\n",
        "while export_task.active():\n",
        "  print(strftime(\"%x %X\"), export_task.status())\n",
        "  sleep(10)\n",
        "\n",
        "print(\"Finished:\", strftime(\"%x %X\"))\n",
        "\n",
        "print('Export should now be visible in Drive at path:\\n',short_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zhMxo32fmnT"
      },
      "source": [
        "# **Pair Landsat with *in-situ* data**\n",
        "This next section is optional, but allows you to compare any field data you may have to the Landsat data produced here. This section uses the CSV exported above.<p>\n",
        "In order for this to run successfully, your data must be in CSV format and have the following headers/columns:\n",
        "\n",
        "\n",
        "*  `datetime`: Date and time of each temperature reading in a recognized strftime format\n",
        "*  `temp_degC`: an integer or float number representing temperature in degrees Celcius\n",
        "*  `location`: for in-lake statistics, a column with lake zone names (string format, no special characters); can be all the same for a single output or differentiated by lake zones/sensors\n",
        "* `depth_m`: as an integer or float number, for statistics about the depth of sensors for matched scenes\n",
        "\n",
        "The file may have additional columns, but the above are mandatory fields. To see an example dataset, click here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APg3jpUagGTf"
      },
      "source": [
        "## Upload CSV of *insitu* data\n",
        "Choose an option below. For more ways to import data, see [here](https://colab.research.google.com/notebooks/io.ipynb)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Gg7oRYPGqF",
        "cellView": "form"
      },
      "source": [
        "#@title Please provide some information about your data\n",
        "#@markdown What timezone is your data? <a href=\"https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\" target=\"_blank\">Olson Times Wikipedia Page</a> Specifically, you need to indicate the UTC offset for your data for proper pairing, including whether or not your data timestamp observes Daylight Savings Time. Enter the text for the matching UTC offset and DST behavior from the 'TZ database name' column in the linked table. Note that the sign of the GMT offset is intentionally inverted from the UTC offset.\n",
        "insitu_timezone = \"Etc/GMT+5\" #@param {type:\"string\"}\n",
        "#@markdown How is your datetime formatted? Please use <a href='https://strftime.org/' target='_blank'>strftime</a> format.\n",
        "datetime_format = \"%Y-%m-%d %H:%M:%S\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X-Eyd1aFf7_",
        "cellView": "form"
      },
      "source": [
        "#@title Option 1: Link to raw CSV from Github\n",
        "pasted_path = \"https://raw.githubusercontent.com/cherrickunh/ids-ne-lakes/master/data/archive/all_temp_data_v2021May14.csv?token=AC6S5HCH7JBBA5H6XIHI2W3BQPMWO\" #@param {type:\"string\"}\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)\n",
        "#is_df[\"datetime\"] = pd.to_datetime(is_df[\"datetime\"])\n",
        "print(\"dataframe created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oBcVtwu42-hG"
      },
      "source": [
        "#@title Option 2: Upload data from your Google Drive folder\n",
        "#@markdown using the folder icon on the left hand side, navigate to the file in\n",
        "#@markdown your Google Drive (named 'drive' here), click on the three vertical dots to the right of the file\n",
        "#@markdown and click on 'copy file path'. Paste the copied path below.\n",
        "\n",
        "pasted_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)\n",
        "print(\"dataframe created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHul1TwpD-go",
        "cellView": "form"
      },
      "source": [
        "#@title Option 3: Upload data from your local file system to Colab\n",
        "#@markdown (temporary while connected to current Colab runtime session)\n",
        "#@markdown Run this cell to upload a file\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print(\"Paste this in the box below:\\n/content/{name}\".format(name=fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52agyeVcKb8Z",
        "cellView": "form"
      },
      "source": [
        "pasted_path = \"/content/insitu_temp_data_v2021-10-20.csv\" #@param {type:\"string\"}\n",
        "\n",
        "#from dateutil.parser import parse\n",
        "\n",
        "#parser = lambda date: datetime.strptime(date, datetime_format)\n",
        "\n",
        "is_df = pd.read_csv(pasted_path)#, parse_dates=[0], date_parser=parser)\n",
        "#is_df[\"datetime\"] = pd.to_datetime(is_df[\"datetime\"])\n",
        "#is_df.set_index('datetime', drop=False, inplace=True)\n",
        "#is_df.index = is_df.index.tz_localize(insitu_timezone).tz_convert(\"UTC\")\n",
        "print(\"dataframe created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnJU34MEO1c-"
      },
      "source": [
        "## Pair *insitu* data with Landsat data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh155YwkBSlc",
        "cellView": "form"
      },
      "source": [
        "#@title Convert local datetime to UTC time\n",
        "#@markdown __Run this block__ to apply conversion of your specified local time to UTC time. \n",
        "insitu_tz = pytz.timezone(insitu_timezone)\n",
        "landsat_tz = pytz.timezone(\"UTC\")\n",
        "\n",
        "def convert_datetime(dt, dtformat=datetime_format):\n",
        "  #converts string format insitu time to a datetime obj\n",
        "  dto = datetime.strptime(dt, datetime_format)\n",
        "  #makes it time aware\n",
        "  dto_local = insitu_tz.localize(dto)\n",
        "  #converts to utc\n",
        "  dto_utc = dto_local.astimezone(landsat_tz)\n",
        "  return dto_utc\n",
        "\n",
        "dtobj_series = is_df['datetime']\n",
        "dtobj_series_conv = dtobj_series.apply(convert_datetime)\n",
        "is_df['datetime_utc'] = dtobj_series_conv\n",
        "\n",
        "print(\"\\nDate/time function imported and datetime converted in dataframe\", strftime(\"%x %X\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2csZXUp0Zlo",
        "cellView": "form"
      },
      "source": [
        "#@markdown Enter the window of time (in minutes) from Landsat flyover where *insitu* data should be included. For example, `timewindow = 30` will include any data within 60 minutes of Landsat flyover (+/- 30 minutes)\n",
        "timewindow = 30 #@param {type:\"number\"\n",
        "\n",
        "cwd = output_dir\n",
        "os.chdir(cwd)\n",
        "\n",
        "outfile = (file_prefix + \"_temp_landsat_paired.csv\")\n",
        "\n",
        "print(f\"Time window: +- {timewindow} minutes\")\n",
        "print(f\"In-situ time Zone: {insitu_timezone}\")\n",
        "print(\"\\nLandsat input file:\\n\", os.path.join(cwd,temp_filename+\".csv\"))\n",
        "print(\"\\nOutput file will be saved to:\\n\", os.path.join(cwd,outfile))\n",
        "\n",
        "min_sec = timewindow * 60\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYfj7ThEZsFl",
        "cellView": "form"
      },
      "source": [
        "#@markdown The following codeblock compares the datasets and generate statistics \n",
        "#@markdown for in-lake data that matches Landsat flyovers.<br> \n",
        "#@markdown It also uses a subfolder called 'ancillary' (and creates the folder if needed) \n",
        "#@markdown that keeps a record of any insitu data points that are used for each Landsat scene.\n",
        "#@markdown If you have a large number of scenes to pair and/or a large dataframe of\n",
        "#@markdown in-situ data, this step may take some time. Timestamps that print below are \n",
        "#@markdown the Landast scene acquisition times where there are in-situ matches.\n",
        "\n",
        "print(\"Start:\", strftime(\"%x %X\"))\n",
        "file_name = (output_dir + '/' + temp_filename + '.csv')\n",
        "gee_csv = pd.read_csv(file_name)\n",
        "insitu_csv = is_df\n",
        "\n",
        "#make 'ancillary' folder in defined directory\n",
        "if not os.path.exists(os.path.join(cwd, 'ancillary')):\n",
        "    os.makedirs(os.path.join(cwd, 'ancillary'))\n",
        "print('Matched records files will be visible at ', cwd+ '/ancillary')\n",
        "\n",
        "#filter GEE output so that it's only as recent as minimum in-situ data\n",
        "insitumin = min(insitu_csv.datetime_utc)\n",
        "insitumax = max(insitu_csv.datetime_utc)\n",
        "\n",
        "def conv_lstime(i):\n",
        "  return pd.Timestamp(i, unit = 'ms', tz = 'utc')\n",
        "\n",
        "gee_csv['landsat_time_utc'] = gee_csv.landsat_time.apply(conv_lstime)\n",
        "\n",
        "gee_overlap = gee_csv[gee_csv.landsat_time_utc>insitumin]\n",
        "gee_overlap_fin = gee_overlap[gee_overlap.landsat_time_utc<insitumax]\n",
        "#reindex filtered dataset\n",
        "gee_overlap_fin = gee_overlap_fin.reset_index(drop=True)\n",
        "\n",
        "gee_datelist = gee_overlap_fin[\"landsat_time_utc\"]\n",
        "\n",
        "# function to convert dt_delta to seconds\n",
        "def delta_tosec(i):\n",
        "  total_secs = i.seconds + i.days*24*60*60\n",
        "  return total_secs\n",
        "\n",
        "for i in range(0,len(gee_datelist)):\n",
        "  scene = gee_overlap_fin['system:index'][i][-20:]\n",
        "  landsattime = gee_overlap_fin['landsat_time_utc'][i]\n",
        "\n",
        "  # create a df of insitu data that are observed within the user-specified cutoff \n",
        "\n",
        "  #calculate time delta for each obs\n",
        "  insitu_csv['dt_delta'] = landsattime - insitu_csv['datetime_utc']\n",
        "  insitu_csv['delta_secs'] = insitu_csv.dt_delta.apply(delta_tosec)  \n",
        "  same_time = insitu_csv[(abs(insitu_csv.delta_secs) <= min_sec)]\n",
        "  \n",
        "  #if the dataframe is not empty, summarize the results\n",
        "  if same_time.shape[0]>0:\n",
        "    print(landsattime)\n",
        "    gee_overlap_fin.loc[i, \"scene\"] = scene\n",
        "    gee_overlap_fin.loc[i, \"is_temp_avg\"] = same_time[\"temp_degC\"].mean()\n",
        "    gee_overlap_fin.loc[i, \"is_temp_stdev\"] = same_time[\"temp_degC\"].std()\n",
        "    gee_overlap_fin.loc[i, \"is_depth_avg\"] = same_time[\"depth_m\"].mean()\n",
        "    gee_overlap_fin.loc[i, \"is_depth_stdev\"] = same_time[\"depth_m\"].std()\n",
        "    gee_overlap_fin.loc[i, \"is_temp_med\"] = same_time[\"temp_degC\"].median()\n",
        "    gee_overlap_fin.loc[i, \"insitu_count\"] = same_time.shape[0]\n",
        "\n",
        "    site_stats = same_time.groupby(['location'])['temp_degC'].agg(['median', 'mean', 'std', 'count'])\n",
        "\n",
        "    sites = site_stats.axes[0]\n",
        "    stats = site_stats.axes[1]\n",
        "\n",
        "    for site in sites:\n",
        "        for stat in stats:\n",
        "            newcol = \"{0}_{1}\".format(str(site), str(stat))\n",
        "            gee_overlap_fin.loc[i, newcol] = site_stats[stat][site].item()\n",
        "\n",
        "    if same_time.shape[0] > 0:\n",
        "        same_time.to_csv(os.path.join(cwd, 'ancillary', scene + \".csv\"))\n",
        "\n",
        "out_csv = gee_overlap_fin[gee_overlap_fin[\"insitu_count\"] > 0]\n",
        "out_csv.to_csv(os.path.join(cwd, outfile))\n",
        "(\"Finished:\", strftime(\"%x %X\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz3nZJqn7R5G"
      },
      "source": [
        "##You are all set. Now go analyze your lake surface temperature data!\n",
        "\n"
      ]
    }
  ]
}